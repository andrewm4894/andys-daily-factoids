import { OpenAI, AzureOpenAI } from 'openai';
import { Buffer } from 'buffer';
import * as uuid from 'uuid';
import { v4 } from 'uuid';
import { wrapLanguageModel } from 'ai';
import AnthropicOriginal from '@anthropic-ai/sdk';
import { GoogleGenAI } from '@google/genai';

var version = "6.4.1";

// limit large outputs by truncating to 200kb (approx 200k bytes)
const MAX_OUTPUT_SIZE = 200000;
const STRING_FORMAT = 'utf8';
const getModelParams = params => {
  if (!params) {
    return {};
  }
  const modelParams = {};
  const paramKeys = ['temperature', 'max_tokens', 'max_completion_tokens', 'top_p', 'frequency_penalty', 'presence_penalty', 'n', 'stop', 'stream', 'streaming'];
  for (const key of paramKeys) {
    if (key in params && params[key] !== undefined) {
      modelParams[key] = params[key];
    }
  }
  return modelParams;
};
const formatResponseAnthropic = response => {
  const output = [];
  const content = [];
  for (const choice of response.content ?? []) {
    if (choice?.type === 'text' && choice?.text) {
      content.push({
        type: 'text',
        text: choice.text
      });
    } else if (choice?.type === 'tool_use' && choice?.name && choice?.id) {
      content.push({
        type: 'function',
        id: choice.id,
        function: {
          name: choice.name,
          arguments: choice.input || {}
        }
      });
    }
  }
  if (content.length > 0) {
    output.push({
      role: 'assistant',
      content
    });
  }
  return output;
};
const formatResponseOpenAI = response => {
  const output = [];
  if (response.choices) {
    for (const choice of response.choices) {
      const content = [];
      let role = 'assistant';
      if (choice.message) {
        if (choice.message.role) {
          role = choice.message.role;
        }
        if (choice.message.content) {
          content.push({
            type: 'text',
            text: choice.message.content
          });
        }
        if (choice.message.tool_calls) {
          for (const toolCall of choice.message.tool_calls) {
            content.push({
              type: 'function',
              id: toolCall.id,
              function: {
                name: toolCall.function.name,
                arguments: toolCall.function.arguments
              }
            });
          }
        }
      }
      if (content.length > 0) {
        output.push({
          role,
          content
        });
      }
    }
  }
  // Handle Responses API format
  if (response.output) {
    const content = [];
    let role = 'assistant';
    for (const item of response.output) {
      if (item.type === 'message') {
        role = item.role;
        if (item.content && Array.isArray(item.content)) {
          for (const contentItem of item.content) {
            if (contentItem.type === 'output_text' && contentItem.text) {
              content.push({
                type: 'text',
                text: contentItem.text
              });
            } else if (contentItem.text) {
              content.push({
                type: 'text',
                text: contentItem.text
              });
            } else if (contentItem.type === 'input_image' && contentItem.image_url) {
              content.push({
                type: 'image',
                image: contentItem.image_url
              });
            }
          }
        } else if (item.content) {
          content.push({
            type: 'text',
            text: String(item.content)
          });
        }
      } else if (item.type === 'function_call') {
        content.push({
          type: 'function',
          id: item.call_id || item.id || '',
          function: {
            name: item.name,
            arguments: item.arguments || {}
          }
        });
      }
    }
    if (content.length > 0) {
      output.push({
        role,
        content
      });
    }
  }
  return output;
};
const formatResponseGemini = response => {
  const output = [];
  if (response.candidates && Array.isArray(response.candidates)) {
    for (const candidate of response.candidates) {
      if (candidate.content && candidate.content.parts) {
        const content = [];
        for (const part of candidate.content.parts) {
          if (part.text) {
            content.push({
              type: 'text',
              text: part.text
            });
          } else if (part.functionCall) {
            content.push({
              type: 'function',
              function: {
                name: part.functionCall.name,
                arguments: part.functionCall.args
              }
            });
          }
        }
        if (content.length > 0) {
          output.push({
            role: 'assistant',
            content
          });
        }
      } else if (candidate.text) {
        output.push({
          role: 'assistant',
          content: [{
            type: 'text',
            text: candidate.text
          }]
        });
      }
    }
  } else if (response.text) {
    output.push({
      role: 'assistant',
      content: [{
        type: 'text',
        text: response.text
      }]
    });
  }
  return output;
};
const mergeSystemPrompt = (params, provider) => {
  {
    const messages = params.messages || [];
    if (!params.system) {
      return messages;
    }
    const systemMessage = params.system;
    return [{
      role: 'system',
      content: systemMessage
    }, ...messages];
  }
};
const withPrivacyMode = (client, privacyMode, input) => {
  return client.privacy_mode || privacyMode ? null : input;
};
function toSafeString(input) {
  if (input === undefined || input === null) {
    return '';
  }
  if (typeof input === 'string') {
    return input;
  }
  try {
    return JSON.stringify(input);
  } catch {
    console.warn('Failed to stringify input', input);
    return '';
  }
}
const truncate = input => {
  const str = toSafeString(input);
  if (str === '') {
    return '';
  }
  // Check if we need to truncate and ensure STRING_FORMAT is respected
  const encoder = new TextEncoder();
  const buffer = encoder.encode(str);
  if (buffer.length <= MAX_OUTPUT_SIZE) {
    // Ensure STRING_FORMAT is respected
    return new TextDecoder(STRING_FORMAT).decode(buffer);
  }
  // Truncate the buffer and ensure a valid string is returned
  const truncatedBuffer = buffer.slice(0, MAX_OUTPUT_SIZE);
  // fatal: false means we get U+FFFD at the end if truncation broke the encoding
  const decoder = new TextDecoder(STRING_FORMAT, {
    fatal: false
  });
  let truncatedStr = decoder.decode(truncatedBuffer);
  if (truncatedStr.endsWith('\uFFFD')) {
    truncatedStr = truncatedStr.slice(0, -1);
  }
  return `${truncatedStr}... [truncated]`;
};
/**
 * Extract available tool calls from the request parameters.
 * These are the tools provided to the LLM, not the tool calls in the response.
 */
const extractAvailableToolCalls = (provider, params) => {
  if (provider === 'anthropic') {
    if (params.tools) {
      return params.tools;
    }
    return null;
  } else if (provider === 'gemini') {
    if (params.config && params.config.tools) {
      return params.config.tools;
    }
    return null;
  } else if (provider === 'openai') {
    if (params.tools) {
      return params.tools;
    }
    return null;
  } else if (provider === 'vercel') {
    if (params.tools) {
      return params.tools;
    }
    return null;
  }
  return null;
};
var AIEvent;
(function (AIEvent) {
  AIEvent["Generation"] = "$ai_generation";
  AIEvent["Embedding"] = "$ai_embedding";
})(AIEvent || (AIEvent = {}));
function sanitizeValues(obj) {
  if (obj === undefined || obj === null) {
    return obj;
  }
  const jsonSafe = JSON.parse(JSON.stringify(obj));
  if (typeof jsonSafe === 'string') {
    return Buffer.from(jsonSafe, STRING_FORMAT).toString(STRING_FORMAT);
  } else if (Array.isArray(jsonSafe)) {
    return jsonSafe.map(sanitizeValues);
  } else if (jsonSafe && typeof jsonSafe === 'object') {
    return Object.fromEntries(Object.entries(jsonSafe).map(([k, v]) => [k, sanitizeValues(v)]));
  }
  return jsonSafe;
}
const POSTHOG_PARAMS_MAP = {
  posthogDistinctId: 'distinctId',
  posthogTraceId: 'traceId',
  posthogProperties: 'properties',
  posthogPrivacyMode: 'privacyMode',
  posthogGroups: 'groups',
  posthogModelOverride: 'modelOverride',
  posthogProviderOverride: 'providerOverride',
  posthogCostOverride: 'costOverride',
  posthogCaptureImmediate: 'captureImmediate'
};
function extractPosthogParams(body) {
  const providerParams = {};
  const posthogParams = {};
  for (const [key, value] of Object.entries(body)) {
    if (POSTHOG_PARAMS_MAP[key]) {
      posthogParams[POSTHOG_PARAMS_MAP[key]] = value;
    } else if (key.startsWith('posthog')) {
      console.warn(`Unknown Posthog parameter ${key}`);
    } else {
      providerParams[key] = value;
    }
  }
  return {
    providerParams: providerParams,
    posthogParams: addDefaults(posthogParams)
  };
}
function addDefaults(params) {
  return {
    ...params,
    privacyMode: params.privacyMode ?? false,
    traceId: params.traceId ?? v4()
  };
}
const sendEventToPosthog = async ({
  client,
  eventType = AIEvent.Generation,
  distinctId,
  traceId,
  model,
  provider,
  input,
  output,
  latency,
  baseURL,
  params,
  httpStatus = 200,
  usage = {},
  isError = false,
  error,
  tools,
  captureImmediate = false
}) => {
  if (!client.capture) {
    return Promise.resolve();
  }
  // sanitize input and output for UTF-8 validity
  const safeInput = sanitizeValues(input);
  const safeOutput = sanitizeValues(output);
  const safeError = sanitizeValues(error);
  let errorData = {};
  if (isError) {
    errorData = {
      $ai_is_error: true,
      $ai_error: safeError
    };
  }
  let costOverrideData = {};
  if (params.posthogCostOverride) {
    const inputCostUSD = (params.posthogCostOverride.inputCost ?? 0) * (usage.inputTokens ?? 0);
    const outputCostUSD = (params.posthogCostOverride.outputCost ?? 0) * (usage.outputTokens ?? 0);
    costOverrideData = {
      $ai_input_cost_usd: inputCostUSD,
      $ai_output_cost_usd: outputCostUSD,
      $ai_total_cost_usd: inputCostUSD + outputCostUSD
    };
  }
  const additionalTokenValues = {
    ...(usage.reasoningTokens ? {
      $ai_reasoning_tokens: usage.reasoningTokens
    } : {}),
    ...(usage.cacheReadInputTokens ? {
      $ai_cache_read_input_tokens: usage.cacheReadInputTokens
    } : {}),
    ...(usage.cacheCreationInputTokens ? {
      $ai_cache_creation_input_tokens: usage.cacheCreationInputTokens
    } : {})
  };
  const properties = {
    $ai_lib: 'posthog-ai',
    $ai_lib_version: version,
    $ai_provider: params.posthogProviderOverride ?? provider,
    $ai_model: params.posthogModelOverride ?? model,
    $ai_model_parameters: getModelParams(params),
    $ai_input: withPrivacyMode(client, params.posthogPrivacyMode ?? false, safeInput),
    $ai_output_choices: withPrivacyMode(client, params.posthogPrivacyMode ?? false, safeOutput),
    $ai_http_status: httpStatus,
    $ai_input_tokens: usage.inputTokens ?? 0,
    ...(usage.outputTokens !== undefined ? {
      $ai_output_tokens: usage.outputTokens
    } : {}),
    ...additionalTokenValues,
    $ai_latency: latency,
    $ai_trace_id: traceId,
    $ai_base_url: baseURL,
    ...params.posthogProperties,
    ...(distinctId ? {} : {
      $process_person_profile: false
    }),
    ...(tools ? {
      $ai_tools: tools
    } : {}),
    ...errorData,
    ...costOverrideData
  };
  const event = {
    distinctId: distinctId ?? traceId,
    event: eventType,
    properties,
    groups: params.posthogGroups
  };
  if (captureImmediate) {
    // await capture promise to send single event in serverless environments
    await client.captureImmediate(event);
  } else {
    client.capture(event);
  }
};

// Type guards for safer type checking
const isString = value => {
  return typeof value === 'string';
};
const isObject = value => {
  return value !== null && typeof value === 'object' && !Array.isArray(value);
};

const REDACTED_IMAGE_PLACEHOLDER = '[base64 image redacted]';
// ============================================
// Base64 Detection Helpers
// ============================================
const isBase64DataUrl = str => {
  return /^data:([^;]+);base64,/.test(str);
};
const isValidUrl = str => {
  try {
    new URL(str);
    return true;
  } catch {
    // Not an absolute URL, check if it's a relative URL or path
    return str.startsWith('/') || str.startsWith('./') || str.startsWith('../');
  }
};
const isRawBase64 = str => {
  // Skip if it's a valid URL or path
  if (isValidUrl(str)) {
    return false;
  }
  // Check if it's a valid base64 string
  // Base64 images are typically at least a few hundred chars, but we'll be conservative
  return str.length > 20 && /^[A-Za-z0-9+/]+=*$/.test(str);
};
function redactBase64DataUrl(str) {
  if (!isString(str)) return str;
  // Check for data URL format
  if (isBase64DataUrl(str)) {
    return REDACTED_IMAGE_PLACEHOLDER;
  }
  // Check for raw base64 (Vercel sends raw base64 for inline images)
  if (isRawBase64(str)) {
    return REDACTED_IMAGE_PLACEHOLDER;
  }
  return str;
}
const processMessages = (messages, transformContent) => {
  if (!messages) return messages;
  const processContent = content => {
    if (typeof content === 'string') return content;
    if (!content) return content;
    if (Array.isArray(content)) {
      return content.map(transformContent);
    }
    // Handle single object content
    return transformContent(content);
  };
  const processMessage = msg => {
    if (!isObject(msg) || !('content' in msg)) return msg;
    return {
      ...msg,
      content: processContent(msg.content)
    };
  };
  // Handle both arrays and single messages
  if (Array.isArray(messages)) {
    return messages.map(processMessage);
  }
  return processMessage(messages);
};
// ============================================
// Provider-Specific Image Sanitizers
// ============================================
const sanitizeOpenAIImage = item => {
  if (!isObject(item)) return item;
  // Handle image_url format
  if (item.type === 'image_url' && 'image_url' in item && isObject(item.image_url) && 'url' in item.image_url) {
    return {
      ...item,
      image_url: {
        ...item.image_url,
        url: redactBase64DataUrl(item.image_url.url)
      }
    };
  }
  return item;
};
const sanitizeOpenAIResponseImage = item => {
  if (!isObject(item)) return item;
  // Handle input_image format
  if (item.type === 'input_image' && 'image_url' in item) {
    return {
      ...item,
      image_url: redactBase64DataUrl(item.image_url)
    };
  }
  return item;
};
const sanitizeAnthropicImage = item => {
  if (!isObject(item)) return item;
  // Handle Anthropic's image format
  if (item.type === 'image' && 'source' in item && isObject(item.source) && item.source.type === 'base64' && 'data' in item.source) {
    return {
      ...item,
      source: {
        ...item.source,
        data: REDACTED_IMAGE_PLACEHOLDER
      }
    };
  }
  return item;
};
const sanitizeGeminiPart = part => {
  if (!isObject(part)) return part;
  // Handle Gemini's inline data format
  if ('inlineData' in part && isObject(part.inlineData) && 'data' in part.inlineData) {
    return {
      ...part,
      inlineData: {
        ...part.inlineData,
        data: REDACTED_IMAGE_PLACEHOLDER
      }
    };
  }
  return part;
};
const processGeminiItem = item => {
  if (!isObject(item)) return item;
  // If it has parts, process them
  if ('parts' in item && item.parts) {
    const parts = Array.isArray(item.parts) ? item.parts.map(sanitizeGeminiPart) : sanitizeGeminiPart(item.parts);
    return {
      ...item,
      parts
    };
  }
  return item;
};
const sanitizeLangChainImage = item => {
  if (!isObject(item)) return item;
  // OpenAI style
  if (item.type === 'image_url' && 'image_url' in item && isObject(item.image_url) && 'url' in item.image_url) {
    return {
      ...item,
      image_url: {
        ...item.image_url,
        url: redactBase64DataUrl(item.image_url.url)
      }
    };
  }
  // Direct image with data field
  if (item.type === 'image' && 'data' in item) {
    return {
      ...item,
      data: redactBase64DataUrl(item.data)
    };
  }
  // Anthropic style
  if (item.type === 'image' && 'source' in item && isObject(item.source) && 'data' in item.source) {
    return {
      ...item,
      source: {
        ...item.source,
        data: redactBase64DataUrl(item.source.data)
      }
    };
  }
  // Google style
  if (item.type === 'media' && 'data' in item) {
    return {
      ...item,
      data: redactBase64DataUrl(item.data)
    };
  }
  return item;
};
// Export individual sanitizers for tree-shaking
const sanitizeOpenAI = data => {
  return processMessages(data, sanitizeOpenAIImage);
};
const sanitizeOpenAIResponse = data => {
  return processMessages(data, sanitizeOpenAIResponseImage);
};
const sanitizeAnthropic = data => {
  return processMessages(data, sanitizeAnthropicImage);
};
const sanitizeGemini = data => {
  // Gemini has a different structure with 'parts' directly on items instead of 'content'
  // So we need custom processing instead of using processMessages
  if (!data) return data;
  if (Array.isArray(data)) {
    return data.map(processGeminiItem);
  }
  return processGeminiItem(data);
};
const sanitizeLangChain = data => {
  return processMessages(data, sanitizeLangChainImage);
};

const Chat = OpenAI.Chat;
const Completions = Chat.Completions;
const Responses = OpenAI.Responses;
const Embeddings = OpenAI.Embeddings;
class PostHogOpenAI extends OpenAI {
  constructor(config) {
    const {
      posthog,
      ...openAIConfig
    } = config;
    super(openAIConfig);
    this.phClient = posthog;
    this.chat = new WrappedChat$1(this, this.phClient);
    this.responses = new WrappedResponses$1(this, this.phClient);
    this.embeddings = new WrappedEmbeddings$1(this, this.phClient);
  }
}
let WrappedChat$1 = class WrappedChat extends Chat {
  constructor(parentClient, phClient) {
    super(parentClient);
    this.completions = new WrappedCompletions$1(parentClient, phClient);
  }
};
let WrappedCompletions$1 = class WrappedCompletions extends Completions {
  constructor(client, phClient) {
    super(client);
    this.phClient = phClient;
    this.baseURL = client.baseURL;
  }
  // --- Implementation Signature
  create(body, options) {
    const {
      providerParams: openAIParams,
      posthogParams
    } = extractPosthogParams(body);
    const startTime = Date.now();
    const parentPromise = super.create(openAIParams, options);
    if (openAIParams.stream) {
      return parentPromise.then(value => {
        if ('tee' in value) {
          const [stream1, stream2] = value.tee();
          (async () => {
            try {
              const contentBlocks = [];
              let accumulatedContent = '';
              let usage = {
                inputTokens: 0,
                outputTokens: 0
              };
              // Map to track in-progress tool calls
              const toolCallsInProgress = new Map();
              for await (const chunk of stream1) {
                const choice = chunk?.choices?.[0];
                // Handle text content
                const deltaContent = choice?.delta?.content;
                if (deltaContent) {
                  accumulatedContent += deltaContent;
                }
                // Handle tool calls
                const deltaToolCalls = choice?.delta?.tool_calls;
                if (deltaToolCalls && Array.isArray(deltaToolCalls)) {
                  for (const toolCall of deltaToolCalls) {
                    const index = toolCall.index;
                    if (index !== undefined) {
                      if (!toolCallsInProgress.has(index)) {
                        // New tool call
                        toolCallsInProgress.set(index, {
                          id: toolCall.id || '',
                          name: toolCall.function?.name || '',
                          arguments: ''
                        });
                      }
                      const inProgressCall = toolCallsInProgress.get(index);
                      if (inProgressCall) {
                        // Update tool call data
                        if (toolCall.id) {
                          inProgressCall.id = toolCall.id;
                        }
                        if (toolCall.function?.name) {
                          inProgressCall.name = toolCall.function.name;
                        }
                        if (toolCall.function?.arguments) {
                          inProgressCall.arguments += toolCall.function.arguments;
                        }
                      }
                    }
                  }
                }
                // Handle usage information
                if (chunk.usage) {
                  usage = {
                    inputTokens: chunk.usage.prompt_tokens ?? 0,
                    outputTokens: chunk.usage.completion_tokens ?? 0,
                    reasoningTokens: chunk.usage.completion_tokens_details?.reasoning_tokens ?? 0,
                    cacheReadInputTokens: chunk.usage.prompt_tokens_details?.cached_tokens ?? 0
                  };
                }
              }
              // Build final content blocks
              if (accumulatedContent) {
                contentBlocks.push({
                  type: 'text',
                  text: accumulatedContent
                });
              }
              // Add completed tool calls to content blocks
              for (const toolCall of toolCallsInProgress.values()) {
                if (toolCall.name) {
                  contentBlocks.push({
                    type: 'function',
                    id: toolCall.id,
                    function: {
                      name: toolCall.name,
                      arguments: toolCall.arguments
                    }
                  });
                }
              }
              // Format output to match non-streaming version
              const formattedOutput = contentBlocks.length > 0 ? [{
                role: 'assistant',
                content: contentBlocks
              }] : [{
                role: 'assistant',
                content: [{
                  type: 'text',
                  text: ''
                }]
              }];
              const latency = (Date.now() - startTime) / 1000;
              const availableTools = extractAvailableToolCalls('openai', openAIParams);
              await sendEventToPosthog({
                client: this.phClient,
                ...posthogParams,
                model: openAIParams.model,
                provider: 'openai',
                input: sanitizeOpenAI(openAIParams.messages),
                output: formattedOutput,
                latency,
                baseURL: this.baseURL,
                params: body,
                httpStatus: 200,
                usage,
                tools: availableTools
              });
            } catch (error) {
              const httpStatus = error && typeof error === 'object' && 'status' in error ? error.status ?? 500 : 500;
              await sendEventToPosthog({
                client: this.phClient,
                ...posthogParams,
                model: openAIParams.model,
                provider: 'openai',
                input: sanitizeOpenAI(openAIParams.messages),
                output: [],
                latency: 0,
                baseURL: this.baseURL,
                params: body,
                httpStatus,
                usage: {
                  inputTokens: 0,
                  outputTokens: 0
                },
                isError: true,
                error: JSON.stringify(error)
              });
            }
          })();
          // Return the other stream to the user
          return stream2;
        }
        return value;
      });
    } else {
      const wrappedPromise = parentPromise.then(async result => {
        if ('choices' in result) {
          const latency = (Date.now() - startTime) / 1000;
          const availableTools = extractAvailableToolCalls('openai', openAIParams);
          await sendEventToPosthog({
            client: this.phClient,
            ...posthogParams,
            model: openAIParams.model,
            provider: 'openai',
            input: sanitizeOpenAI(openAIParams.messages),
            output: formatResponseOpenAI(result),
            latency,
            baseURL: this.baseURL,
            params: body,
            httpStatus: 200,
            usage: {
              inputTokens: result.usage?.prompt_tokens ?? 0,
              outputTokens: result.usage?.completion_tokens ?? 0,
              reasoningTokens: result.usage?.completion_tokens_details?.reasoning_tokens ?? 0,
              cacheReadInputTokens: result.usage?.prompt_tokens_details?.cached_tokens ?? 0
            },
            tools: availableTools
          });
        }
        return result;
      }, async error => {
        const httpStatus = error && typeof error === 'object' && 'status' in error ? error.status ?? 500 : 500;
        await sendEventToPosthog({
          client: this.phClient,
          ...posthogParams,
          model: String(openAIParams.model ?? ''),
          provider: 'openai',
          input: sanitizeOpenAI(openAIParams.messages),
          output: [],
          latency: 0,
          baseURL: this.baseURL,
          params: body,
          httpStatus,
          usage: {
            inputTokens: 0,
            outputTokens: 0
          },
          isError: true,
          error: JSON.stringify(error)
        });
        throw error;
      });
      return wrappedPromise;
    }
  }
};
let WrappedResponses$1 = class WrappedResponses extends Responses {
  constructor(client, phClient) {
    super(client);
    this.phClient = phClient;
    this.baseURL = client.baseURL;
  }
  // --- Implementation Signature
  create(body, options) {
    const {
      providerParams: openAIParams,
      posthogParams
    } = extractPosthogParams(body);
    const startTime = Date.now();
    const parentPromise = super.create(openAIParams, options);
    if (openAIParams.stream) {
      return parentPromise.then(value => {
        if ('tee' in value && typeof value.tee === 'function') {
          const [stream1, stream2] = value.tee();
          (async () => {
            try {
              let finalContent = [];
              let usage = {
                inputTokens: 0,
                outputTokens: 0
              };
              for await (const chunk of stream1) {
                if (chunk.type === 'response.completed' && 'response' in chunk && chunk.response?.output && chunk.response.output.length > 0) {
                  finalContent = chunk.response.output;
                }
                if ('response' in chunk && chunk.response?.usage) {
                  usage = {
                    inputTokens: chunk.response.usage.input_tokens ?? 0,
                    outputTokens: chunk.response.usage.output_tokens ?? 0,
                    reasoningTokens: chunk.response.usage.output_tokens_details?.reasoning_tokens ?? 0,
                    cacheReadInputTokens: chunk.response.usage.input_tokens_details?.cached_tokens ?? 0
                  };
                }
              }
              const latency = (Date.now() - startTime) / 1000;
              const availableTools = extractAvailableToolCalls('openai', openAIParams);
              await sendEventToPosthog({
                client: this.phClient,
                ...posthogParams,
                //@ts-expect-error
                model: openAIParams.model,
                provider: 'openai',
                input: sanitizeOpenAIResponse(openAIParams.input),
                output: finalContent,
                latency,
                baseURL: this.baseURL,
                params: body,
                httpStatus: 200,
                usage,
                tools: availableTools
              });
            } catch (error) {
              const httpStatus = error && typeof error === 'object' && 'status' in error ? error.status ?? 500 : 500;
              await sendEventToPosthog({
                client: this.phClient,
                ...posthogParams,
                //@ts-expect-error
                model: openAIParams.model,
                provider: 'openai',
                input: sanitizeOpenAIResponse(openAIParams.input),
                output: [],
                latency: 0,
                baseURL: this.baseURL,
                params: body,
                httpStatus,
                usage: {
                  inputTokens: 0,
                  outputTokens: 0
                },
                isError: true,
                error: JSON.stringify(error)
              });
            }
          })();
          return stream2;
        }
        return value;
      });
    } else {
      const wrappedPromise = parentPromise.then(async result => {
        if ('output' in result) {
          const latency = (Date.now() - startTime) / 1000;
          const availableTools = extractAvailableToolCalls('openai', openAIParams);
          await sendEventToPosthog({
            client: this.phClient,
            ...posthogParams,
            //@ts-expect-error
            model: openAIParams.model,
            provider: 'openai',
            input: sanitizeOpenAIResponse(openAIParams.input),
            output: formatResponseOpenAI({
              output: result.output
            }),
            latency,
            baseURL: this.baseURL,
            params: body,
            httpStatus: 200,
            usage: {
              inputTokens: result.usage?.input_tokens ?? 0,
              outputTokens: result.usage?.output_tokens ?? 0,
              reasoningTokens: result.usage?.output_tokens_details?.reasoning_tokens ?? 0,
              cacheReadInputTokens: result.usage?.input_tokens_details?.cached_tokens ?? 0
            },
            tools: availableTools
          });
        }
        return result;
      }, async error => {
        const httpStatus = error && typeof error === 'object' && 'status' in error ? error.status ?? 500 : 500;
        await sendEventToPosthog({
          client: this.phClient,
          ...posthogParams,
          model: String(openAIParams.model ?? ''),
          provider: 'openai',
          input: sanitizeOpenAIResponse(openAIParams.input),
          output: [],
          latency: 0,
          baseURL: this.baseURL,
          params: body,
          httpStatus,
          usage: {
            inputTokens: 0,
            outputTokens: 0
          },
          isError: true,
          error: JSON.stringify(error)
        });
        throw error;
      });
      return wrappedPromise;
    }
  }
  parse(body, options) {
    const {
      providerParams: openAIParams,
      posthogParams
    } = extractPosthogParams(body);
    const startTime = Date.now();
    const originalCreate = super.create.bind(this);
    const originalSelf = this;
    const tempCreate = originalSelf.create;
    originalSelf.create = originalCreate;
    try {
      const parentPromise = super.parse(openAIParams, options);
      const wrappedPromise = parentPromise.then(async result => {
        const latency = (Date.now() - startTime) / 1000;
        await sendEventToPosthog({
          client: this.phClient,
          ...posthogParams,
          model: String(openAIParams.model ?? ''),
          provider: 'openai',
          input: sanitizeOpenAIResponse(openAIParams.input),
          output: result.output,
          latency,
          baseURL: this.baseURL,
          params: body,
          httpStatus: 200,
          usage: {
            inputTokens: result.usage?.input_tokens ?? 0,
            outputTokens: result.usage?.output_tokens ?? 0,
            reasoningTokens: result.usage?.output_tokens_details?.reasoning_tokens ?? 0,
            cacheReadInputTokens: result.usage?.input_tokens_details?.cached_tokens ?? 0
          }
        });
        return result;
      }, async error => {
        const httpStatus = error && typeof error === 'object' && 'status' in error ? error.status ?? 500 : 500;
        await sendEventToPosthog({
          client: this.phClient,
          ...posthogParams,
          model: String(openAIParams.model ?? ''),
          provider: 'openai',
          input: sanitizeOpenAIResponse(openAIParams.input),
          output: [],
          latency: 0,
          baseURL: this.baseURL,
          params: body,
          httpStatus,
          usage: {
            inputTokens: 0,
            outputTokens: 0
          },
          isError: true,
          error: JSON.stringify(error)
        });
        throw error;
      });
      return wrappedPromise;
    } finally {
      // Restore our wrapped create method
      originalSelf.create = tempCreate;
    }
  }
};
let WrappedEmbeddings$1 = class WrappedEmbeddings extends Embeddings {
  constructor(client, phClient) {
    super(client);
    this.phClient = phClient;
    this.baseURL = client.baseURL;
  }
  create(body, options) {
    const {
      providerParams: openAIParams,
      posthogParams
    } = extractPosthogParams(body);
    const startTime = Date.now();
    const parentPromise = super.create(openAIParams, options);
    const wrappedPromise = parentPromise.then(async result => {
      const latency = (Date.now() - startTime) / 1000;
      await sendEventToPosthog({
        client: this.phClient,
        ...posthogParams,
        eventType: AIEvent.Embedding,
        model: openAIParams.model,
        provider: 'openai',
        input: withPrivacyMode(this.phClient, posthogParams.privacyMode, openAIParams.input),
        output: null,
        // Embeddings don't have output content
        latency,
        baseURL: this.baseURL,
        params: body,
        httpStatus: 200,
        usage: {
          inputTokens: result.usage?.prompt_tokens ?? 0
        }
      });
      return result;
    }, async error => {
      const httpStatus = error && typeof error === 'object' && 'status' in error ? error.status ?? 500 : 500;
      await sendEventToPosthog({
        client: this.phClient,
        eventType: AIEvent.Embedding,
        ...posthogParams,
        model: openAIParams.model,
        provider: 'openai',
        input: withPrivacyMode(this.phClient, posthogParams.privacyMode, openAIParams.input),
        output: null,
        // Embeddings don't have output content
        latency: 0,
        baseURL: this.baseURL,
        params: body,
        httpStatus,
        usage: {
          inputTokens: 0
        },
        isError: true,
        error: JSON.stringify(error)
      });
      throw error;
    });
    return wrappedPromise;
  }
};

class PostHogAzureOpenAI extends AzureOpenAI {
  constructor(config) {
    const {
      posthog,
      ...openAIConfig
    } = config;
    super(openAIConfig);
    this.phClient = posthog;
    this.chat = new WrappedChat(this, this.phClient);
    this.embeddings = new WrappedEmbeddings(this, this.phClient);
  }
}
class WrappedChat extends AzureOpenAI.Chat {
  constructor(parentClient, phClient) {
    super(parentClient);
    this.completions = new WrappedCompletions(parentClient, phClient);
  }
}
class WrappedCompletions extends AzureOpenAI.Chat.Completions {
  constructor(client, phClient) {
    super(client);
    this.phClient = phClient;
    this.baseURL = client.baseURL;
  }
  // --- Implementation Signature
  create(body, options) {
    const {
      providerParams: openAIParams,
      posthogParams
    } = extractPosthogParams(body);
    const startTime = Date.now();
    const parentPromise = super.create(openAIParams, options);
    if (openAIParams.stream) {
      return parentPromise.then(value => {
        if ('tee' in value) {
          const [stream1, stream2] = value.tee();
          (async () => {
            try {
              const contentBlocks = [];
              let accumulatedContent = '';
              let usage = {
                inputTokens: 0,
                outputTokens: 0
              };
              // Map to track in-progress tool calls
              const toolCallsInProgress = new Map();
              for await (const chunk of stream1) {
                const choice = chunk?.choices?.[0];
                // Handle text content
                const deltaContent = choice?.delta?.content;
                if (deltaContent) {
                  accumulatedContent += deltaContent;
                }
                // Handle tool calls
                const deltaToolCalls = choice?.delta?.tool_calls;
                if (deltaToolCalls && Array.isArray(deltaToolCalls)) {
                  for (const toolCall of deltaToolCalls) {
                    const index = toolCall.index;
                    if (index !== undefined) {
                      if (!toolCallsInProgress.has(index)) {
                        // New tool call
                        toolCallsInProgress.set(index, {
                          id: toolCall.id || '',
                          name: toolCall.function?.name || '',
                          arguments: ''
                        });
                      }
                      const inProgressCall = toolCallsInProgress.get(index);
                      if (inProgressCall) {
                        // Update tool call data
                        if (toolCall.id) {
                          inProgressCall.id = toolCall.id;
                        }
                        if (toolCall.function?.name) {
                          inProgressCall.name = toolCall.function.name;
                        }
                        if (toolCall.function?.arguments) {
                          inProgressCall.arguments += toolCall.function.arguments;
                        }
                      }
                    }
                  }
                }
                // Handle usage information
                if (chunk.usage) {
                  usage = {
                    inputTokens: chunk.usage.prompt_tokens ?? 0,
                    outputTokens: chunk.usage.completion_tokens ?? 0,
                    reasoningTokens: chunk.usage.completion_tokens_details?.reasoning_tokens ?? 0,
                    cacheReadInputTokens: chunk.usage.prompt_tokens_details?.cached_tokens ?? 0
                  };
                }
              }
              // Build final content blocks
              if (accumulatedContent) {
                contentBlocks.push({
                  type: 'text',
                  text: accumulatedContent
                });
              }
              // Add completed tool calls to content blocks
              for (const toolCall of toolCallsInProgress.values()) {
                if (toolCall.name) {
                  contentBlocks.push({
                    type: 'function',
                    id: toolCall.id,
                    function: {
                      name: toolCall.name,
                      arguments: toolCall.arguments
                    }
                  });
                }
              }
              // Format output to match non-streaming version
              const formattedOutput = contentBlocks.length > 0 ? [{
                role: 'assistant',
                content: contentBlocks
              }] : [{
                role: 'assistant',
                content: [{
                  type: 'text',
                  text: ''
                }]
              }];
              const latency = (Date.now() - startTime) / 1000;
              await sendEventToPosthog({
                client: this.phClient,
                ...posthogParams,
                model: openAIParams.model,
                provider: 'azure',
                input: openAIParams.messages,
                output: formattedOutput,
                latency,
                baseURL: this.baseURL,
                params: body,
                httpStatus: 200,
                usage
              });
            } catch (error) {
              const httpStatus = error && typeof error === 'object' && 'status' in error ? error.status ?? 500 : 500;
              await sendEventToPosthog({
                client: this.phClient,
                ...posthogParams,
                model: openAIParams.model,
                provider: 'azure',
                input: openAIParams.messages,
                output: [],
                latency: 0,
                baseURL: this.baseURL,
                params: body,
                httpStatus,
                usage: {
                  inputTokens: 0,
                  outputTokens: 0
                },
                isError: true,
                error: JSON.stringify(error)
              });
            }
          })();
          // Return the other stream to the user
          return stream2;
        }
        return value;
      });
    } else {
      const wrappedPromise = parentPromise.then(async result => {
        if ('choices' in result) {
          const latency = (Date.now() - startTime) / 1000;
          await sendEventToPosthog({
            client: this.phClient,
            ...posthogParams,
            model: openAIParams.model,
            provider: 'azure',
            input: openAIParams.messages,
            output: formatResponseOpenAI(result),
            latency,
            baseURL: this.baseURL,
            params: body,
            httpStatus: 200,
            usage: {
              inputTokens: result.usage?.prompt_tokens ?? 0,
              outputTokens: result.usage?.completion_tokens ?? 0,
              reasoningTokens: result.usage?.completion_tokens_details?.reasoning_tokens ?? 0,
              cacheReadInputTokens: result.usage?.prompt_tokens_details?.cached_tokens ?? 0
            }
          });
        }
        return result;
      }, async error => {
        const httpStatus = error && typeof error === 'object' && 'status' in error ? error.status ?? 500 : 500;
        await sendEventToPosthog({
          client: this.phClient,
          ...posthogParams,
          model: openAIParams.model,
          provider: 'azure',
          input: openAIParams.messages,
          output: [],
          latency: 0,
          baseURL: this.baseURL,
          params: body,
          httpStatus,
          usage: {
            inputTokens: 0,
            outputTokens: 0
          },
          isError: true,
          error: JSON.stringify(error)
        });
        throw error;
      });
      return wrappedPromise;
    }
  }
}
class WrappedResponses extends AzureOpenAI.Responses {
  constructor(client, phClient) {
    super(client);
    this.phClient = phClient;
    this.baseURL = client.baseURL;
  }
  // --- Implementation Signature
  create(body, options) {
    const {
      providerParams: openAIParams,
      posthogParams
    } = extractPosthogParams(body);
    const startTime = Date.now();
    const parentPromise = super.create(openAIParams, options);
    if (openAIParams.stream) {
      return parentPromise.then(value => {
        if ('tee' in value && typeof value.tee === 'function') {
          const [stream1, stream2] = value.tee();
          (async () => {
            try {
              let finalContent = [];
              let usage = {
                inputTokens: 0,
                outputTokens: 0
              };
              for await (const chunk of stream1) {
                if (chunk.type === 'response.completed' && 'response' in chunk && chunk.response?.output && chunk.response.output.length > 0) {
                  finalContent = chunk.response.output;
                }
                if ('usage' in chunk && chunk.usage) {
                  usage = {
                    inputTokens: chunk.usage.input_tokens ?? 0,
                    outputTokens: chunk.usage.output_tokens ?? 0,
                    reasoningTokens: chunk.usage.output_tokens_details?.reasoning_tokens ?? 0,
                    cacheReadInputTokens: chunk.usage.input_tokens_details?.cached_tokens ?? 0
                  };
                }
              }
              const latency = (Date.now() - startTime) / 1000;
              await sendEventToPosthog({
                client: this.phClient,
                ...posthogParams,
                //@ts-expect-error
                model: openAIParams.model,
                provider: 'azure',
                input: openAIParams.input,
                output: finalContent,
                latency,
                baseURL: this.baseURL,
                params: body,
                httpStatus: 200,
                usage
              });
            } catch (error) {
              const httpStatus = error && typeof error === 'object' && 'status' in error ? error.status ?? 500 : 500;
              await sendEventToPosthog({
                client: this.phClient,
                ...posthogParams,
                //@ts-expect-error
                model: openAIParams.model,
                provider: 'azure',
                input: openAIParams.input,
                output: [],
                latency: 0,
                baseURL: this.baseURL,
                params: body,
                httpStatus,
                usage: {
                  inputTokens: 0,
                  outputTokens: 0
                },
                isError: true,
                error: JSON.stringify(error)
              });
            }
          })();
          return stream2;
        }
        return value;
      });
    } else {
      const wrappedPromise = parentPromise.then(async result => {
        if ('output' in result) {
          const latency = (Date.now() - startTime) / 1000;
          await sendEventToPosthog({
            client: this.phClient,
            ...posthogParams,
            //@ts-expect-error
            model: openAIParams.model,
            provider: 'azure',
            input: openAIParams.input,
            output: result.output,
            latency,
            baseURL: this.baseURL,
            params: body,
            httpStatus: 200,
            usage: {
              inputTokens: result.usage?.input_tokens ?? 0,
              outputTokens: result.usage?.output_tokens ?? 0,
              reasoningTokens: result.usage?.output_tokens_details?.reasoning_tokens ?? 0,
              cacheReadInputTokens: result.usage?.input_tokens_details?.cached_tokens ?? 0
            }
          });
        }
        return result;
      }, async error => {
        const httpStatus = error && typeof error === 'object' && 'status' in error ? error.status ?? 500 : 500;
        await sendEventToPosthog({
          client: this.phClient,
          ...posthogParams,
          //@ts-expect-error
          model: openAIParams.model,
          provider: 'azure',
          input: openAIParams.input,
          output: [],
          latency: 0,
          baseURL: this.baseURL,
          params: body,
          httpStatus,
          usage: {
            inputTokens: 0,
            outputTokens: 0
          },
          isError: true,
          error: JSON.stringify(error)
        });
        throw error;
      });
      return wrappedPromise;
    }
  }
  parse(body, options) {
    const {
      providerParams: openAIParams,
      posthogParams
    } = extractPosthogParams(body);
    const startTime = Date.now();
    const parentPromise = super.parse(openAIParams, options);
    const wrappedPromise = parentPromise.then(async result => {
      const latency = (Date.now() - startTime) / 1000;
      await sendEventToPosthog({
        client: this.phClient,
        ...posthogParams,
        model: String(openAIParams.model ?? ''),
        provider: 'azure',
        input: openAIParams.input,
        output: result.output,
        latency,
        baseURL: this.baseURL,
        params: body,
        httpStatus: 200,
        usage: {
          inputTokens: result.usage?.input_tokens ?? 0,
          outputTokens: result.usage?.output_tokens ?? 0,
          reasoningTokens: result.usage?.output_tokens_details?.reasoning_tokens ?? 0,
          cacheReadInputTokens: result.usage?.input_tokens_details?.cached_tokens ?? 0
        }
      });
      return result;
    }, async error => {
      await sendEventToPosthog({
        client: this.phClient,
        ...posthogParams,
        model: String(openAIParams.model ?? ''),
        provider: 'azure',
        input: openAIParams.input,
        output: [],
        latency: 0,
        baseURL: this.baseURL,
        params: body,
        httpStatus: error?.status ? error.status : 500,
        usage: {
          inputTokens: 0,
          outputTokens: 0
        },
        isError: true,
        error: JSON.stringify(error)
      });
      throw error;
    });
    return wrappedPromise;
  }
}
class WrappedEmbeddings extends AzureOpenAI.Embeddings {
  constructor(client, phClient) {
    super(client);
    this.phClient = phClient;
    this.baseURL = client.baseURL;
  }
  create(body, options) {
    const {
      providerParams: openAIParams,
      posthogParams
    } = extractPosthogParams(body);
    const startTime = Date.now();
    const parentPromise = super.create(openAIParams, options);
    const wrappedPromise = parentPromise.then(async result => {
      const latency = (Date.now() - startTime) / 1000;
      await sendEventToPosthog({
        client: this.phClient,
        eventType: AIEvent.Embedding,
        ...posthogParams,
        model: openAIParams.model,
        provider: 'azure',
        input: withPrivacyMode(this.phClient, posthogParams.privacyMode, openAIParams.input),
        output: null,
        // Embeddings don't have output content
        latency,
        baseURL: this.baseURL,
        params: body,
        httpStatus: 200,
        usage: {
          inputTokens: result.usage?.prompt_tokens ?? 0
        }
      });
      return result;
    }, async error => {
      const httpStatus = error && typeof error === 'object' && 'status' in error ? error.status ?? 500 : 500;
      await sendEventToPosthog({
        client: this.phClient,
        eventType: AIEvent.Embedding,
        ...posthogParams,
        model: openAIParams.model,
        provider: 'azure',
        input: withPrivacyMode(this.phClient, posthogParams.privacyMode, openAIParams.input),
        output: null,
        latency: 0,
        baseURL: this.baseURL,
        params: body,
        httpStatus,
        usage: {
          inputTokens: 0
        },
        isError: true,
        error: JSON.stringify(error)
      });
      throw error;
    });
    return wrappedPromise;
  }
}

const mapVercelParams = params => {
  return {
    temperature: params.temperature,
    max_output_tokens: params.maxOutputTokens,
    top_p: params.topP,
    frequency_penalty: params.frequencyPenalty,
    presence_penalty: params.presencePenalty,
    stop: params.stopSequences,
    stream: params.stream
  };
};
const mapVercelPrompt = messages => {
  // Map and truncate individual content
  const inputs = messages.map(message => {
    let content;
    // Handle system role which has string content
    if (message.role === 'system') {
      content = [{
        type: 'text',
        text: truncate(String(message.content))
      }];
    } else {
      // Handle other roles which have array content
      if (Array.isArray(message.content)) {
        content = message.content.map(c => {
          if (c.type === 'text') {
            return {
              type: 'text',
              text: truncate(c.text)
            };
          } else if (c.type === 'file') {
            // For file type, check if it's a data URL and redact if needed
            let fileData;
            const contentData = c.data;
            if (contentData instanceof URL) {
              fileData = contentData.toString();
            } else if (isString(contentData)) {
              // Redact base64 data URLs and raw base64 to prevent oversized events
              fileData = redactBase64DataUrl(contentData);
            } else {
              fileData = 'raw files not supported';
            }
            return {
              type: 'file',
              file: fileData,
              mediaType: c.mediaType
            };
          } else if (c.type === 'reasoning') {
            return {
              type: 'reasoning',
              text: truncate(c.reasoning)
            };
          } else if (c.type === 'tool-call') {
            return {
              type: 'tool-call',
              toolCallId: c.toolCallId,
              toolName: c.toolName,
              input: c.input
            };
          } else if (c.type === 'tool-result') {
            return {
              type: 'tool-result',
              toolCallId: c.toolCallId,
              toolName: c.toolName,
              output: c.output,
              isError: c.isError
            };
          }
          return {
            type: 'text',
            text: ''
          };
        });
      } else {
        // Fallback for non-array content
        content = [{
          type: 'text',
          text: truncate(String(message.content))
        }];
      }
    }
    return {
      role: message.role,
      content
    };
  });
  try {
    // Trim the inputs array until its JSON size fits within MAX_OUTPUT_SIZE
    let serialized = JSON.stringify(inputs);
    let removedCount = 0;
    // We need to keep track of the initial size of the inputs array because we're going to be mutating it
    const initialSize = inputs.length;
    for (let i = 0; i < initialSize && Buffer.byteLength(serialized, 'utf8') > MAX_OUTPUT_SIZE; i++) {
      inputs.shift();
      removedCount++;
      serialized = JSON.stringify(inputs);
    }
    if (removedCount > 0) {
      // Add one placeholder to indicate how many were removed
      inputs.unshift({
        role: 'posthog',
        content: `[${removedCount} message${removedCount === 1 ? '' : 's'} removed due to size limit]`
      });
    }
  } catch (error) {
    console.error('Error stringifying inputs', error);
    return [{
      role: 'posthog',
      content: 'An error occurred while processing your request. Please try again.'
    }];
  }
  return inputs;
};
const mapVercelOutput = result => {
  const content = result.map(item => {
    if (item.type === 'text') {
      return {
        type: 'text',
        text: truncate(item.text)
      };
    }
    if (item.type === 'tool-call') {
      return {
        type: 'tool-call',
        id: item.toolCallId,
        function: {
          name: item.toolName,
          arguments: item.args || JSON.stringify(item.arguments || {})
        }
      };
    }
    if (item.type === 'reasoning') {
      return {
        type: 'reasoning',
        text: truncate(item.text)
      };
    }
    if (item.type === 'file') {
      // Handle files similar to input mapping - avoid large base64 data
      let fileData;
      if (item.data instanceof URL) {
        fileData = item.data.toString();
      } else if (typeof item.data === 'string') {
        fileData = redactBase64DataUrl(item.data);
        // If not redacted and still large, replace with size indicator
        if (fileData === item.data && item.data.length > 1000) {
          fileData = `[${item.mediaType} file - ${item.data.length} bytes]`;
        }
      } else {
        fileData = `[binary ${item.mediaType} file]`;
      }
      return {
        type: 'file',
        name: 'generated_file',
        mediaType: item.mediaType,
        data: fileData
      };
    }
    if (item.type === 'source') {
      return {
        type: 'source',
        sourceType: item.sourceType,
        id: item.id,
        url: item.url || '',
        title: item.title || ''
      };
    }
    // Fallback for unknown types - try to extract text if possible
    return {
      type: 'text',
      text: truncate(JSON.stringify(item))
    };
  });
  if (content.length > 0) {
    return [{
      role: 'assistant',
      content: content.length === 1 && content[0].type === 'text' ? content[0].text : content
    }];
  }
  // otherwise stringify and truncate
  try {
    const jsonOutput = JSON.stringify(result);
    return [{
      content: truncate(jsonOutput),
      role: 'assistant'
    }];
  } catch {
    console.error('Error stringifying output');
    return [];
  }
};
const extractProvider = model => {
  const provider = model.provider.toLowerCase();
  const providerName = provider.split('.')[0];
  return providerName;
};
const createInstrumentationMiddleware = (phClient, model, options) => {
  const middleware = {
    wrapGenerate: async ({
      doGenerate,
      params
    }) => {
      const startTime = Date.now();
      const mergedParams = {
        ...options,
        ...mapVercelParams(params)
      };
      const availableTools = extractAvailableToolCalls('vercel', params);
      try {
        const result = await doGenerate();
        const modelId = options.posthogModelOverride ?? (result.response?.modelId ? result.response.modelId : model.modelId);
        const provider = options.posthogProviderOverride ?? extractProvider(model);
        const baseURL = ''; // cannot currently get baseURL from vercel
        const content = mapVercelOutput(result.content);
        const latency = (Date.now() - startTime) / 1000;
        const providerMetadata = result.providerMetadata;
        const additionalTokenValues = {
          ...(providerMetadata?.anthropic ? {
            cacheCreationInputTokens: providerMetadata.anthropic.cacheCreationInputTokens
          } : {})
        };
        const usage = {
          inputTokens: result.usage.inputTokens,
          outputTokens: result.usage.outputTokens,
          reasoningTokens: result.usage.reasoningTokens,
          cacheReadInputTokens: result.usage.cachedInputTokens,
          ...additionalTokenValues
        };
        await sendEventToPosthog({
          client: phClient,
          distinctId: options.posthogDistinctId,
          traceId: options.posthogTraceId ?? v4(),
          model: modelId,
          provider: provider,
          input: options.posthogPrivacyMode ? '' : mapVercelPrompt(params.prompt),
          output: content,
          latency,
          baseURL,
          params: mergedParams,
          httpStatus: 200,
          usage,
          tools: availableTools,
          captureImmediate: options.posthogCaptureImmediate
        });
        return result;
      } catch (error) {
        const modelId = model.modelId;
        await sendEventToPosthog({
          client: phClient,
          distinctId: options.posthogDistinctId,
          traceId: options.posthogTraceId ?? v4(),
          model: modelId,
          provider: model.provider,
          input: options.posthogPrivacyMode ? '' : mapVercelPrompt(params.prompt),
          output: [],
          latency: 0,
          baseURL: '',
          params: mergedParams,
          httpStatus: error?.status ? error.status : 500,
          usage: {
            inputTokens: 0,
            outputTokens: 0
          },
          isError: true,
          error: truncate(JSON.stringify(error)),
          tools: availableTools,
          captureImmediate: options.posthogCaptureImmediate
        });
        throw error;
      }
    },
    wrapStream: async ({
      doStream,
      params
    }) => {
      const startTime = Date.now();
      let generatedText = '';
      let reasoningText = '';
      let usage = {};
      const mergedParams = {
        ...options,
        ...mapVercelParams(params)
      };
      const modelId = options.posthogModelOverride ?? model.modelId;
      const provider = options.posthogProviderOverride ?? extractProvider(model);
      const availableTools = extractAvailableToolCalls('vercel', params);
      const baseURL = ''; // cannot currently get baseURL from vercel
      // Map to track in-progress tool calls
      const toolCallsInProgress = new Map();
      try {
        const {
          stream,
          ...rest
        } = await doStream();
        const transformStream = new TransformStream({
          transform(chunk, controller) {
            // Handle new v5 streaming patterns
            if (chunk.type === 'text-delta') {
              generatedText += chunk.delta;
            }
            if (chunk.type === 'reasoning-delta') {
              reasoningText += chunk.delta; // New in v5
            }
            // Handle tool call chunks
            if (chunk.type === 'tool-input-start') {
              // Initialize a new tool call
              toolCallsInProgress.set(chunk.id, {
                toolCallId: chunk.id,
                toolName: chunk.toolName,
                input: ''
              });
            }
            if (chunk.type === 'tool-input-delta') {
              // Accumulate tool call arguments
              const toolCall = toolCallsInProgress.get(chunk.id);
              if (toolCall) {
                toolCall.input += chunk.delta;
              }
            }
            if (chunk.type === 'tool-input-end') {
              // Tool call is complete, keep it in the map for final processing
              // Nothing specific to do here, the tool call is already complete
            }
            if (chunk.type === 'tool-call') {
              // Direct tool call chunk (complete tool call)
              toolCallsInProgress.set(chunk.toolCallId, {
                toolCallId: chunk.toolCallId,
                toolName: chunk.toolName,
                input: chunk.input
              });
            }
            if (chunk.type === 'finish') {
              const providerMetadata = chunk.providerMetadata;
              const additionalTokenValues = {
                ...(providerMetadata?.anthropic ? {
                  cacheCreationInputTokens: providerMetadata.anthropic.cacheCreationInputTokens
                } : {})
              };
              usage = {
                inputTokens: chunk.usage?.inputTokens,
                outputTokens: chunk.usage?.outputTokens,
                reasoningTokens: chunk.usage?.reasoningTokens,
                cacheReadInputTokens: chunk.usage?.cachedInputTokens,
                ...additionalTokenValues
              };
            }
            controller.enqueue(chunk);
          },
          flush: async () => {
            const latency = (Date.now() - startTime) / 1000;
            // Build content array similar to mapVercelOutput structure
            const content = [];
            if (reasoningText) {
              content.push({
                type: 'reasoning',
                text: truncate(reasoningText)
              });
            }
            if (generatedText) {
              content.push({
                type: 'text',
                text: truncate(generatedText)
              });
            }
            // Add completed tool calls to content
            for (const toolCall of toolCallsInProgress.values()) {
              if (toolCall.toolName) {
                content.push({
                  type: 'tool-call',
                  id: toolCall.toolCallId,
                  function: {
                    name: toolCall.toolName,
                    arguments: toolCall.input
                  }
                });
              }
            }
            // Structure output like mapVercelOutput does
            const output = content.length > 0 ? [{
              role: 'assistant',
              content: content.length === 1 && content[0].type === 'text' ? content[0].text : content
            }] : [];
            await sendEventToPosthog({
              client: phClient,
              distinctId: options.posthogDistinctId,
              traceId: options.posthogTraceId ?? v4(),
              model: modelId,
              provider: provider,
              input: options.posthogPrivacyMode ? '' : mapVercelPrompt(params.prompt),
              output: output,
              latency,
              baseURL,
              params: mergedParams,
              httpStatus: 200,
              usage,
              tools: availableTools,
              captureImmediate: options.posthogCaptureImmediate
            });
          }
        });
        return {
          stream: stream.pipeThrough(transformStream),
          ...rest
        };
      } catch (error) {
        await sendEventToPosthog({
          client: phClient,
          distinctId: options.posthogDistinctId,
          traceId: options.posthogTraceId ?? v4(),
          model: modelId,
          provider: provider,
          input: options.posthogPrivacyMode ? '' : mapVercelPrompt(params.prompt),
          output: [],
          latency: 0,
          baseURL: '',
          params: mergedParams,
          httpStatus: error?.status ? error.status : 500,
          usage: {
            inputTokens: 0,
            outputTokens: 0
          },
          isError: true,
          error: truncate(JSON.stringify(error)),
          tools: availableTools,
          captureImmediate: options.posthogCaptureImmediate
        });
        throw error;
      }
    }
  };
  return middleware;
};
const wrapVercelLanguageModel = (model, phClient, options) => {
  const traceId = options.posthogTraceId ?? v4();
  const middleware = createInstrumentationMiddleware(phClient, model, {
    ...options,
    posthogTraceId: traceId,
    posthogDistinctId: options.posthogDistinctId
  });
  const wrappedModel = wrapLanguageModel({
    model,
    middleware
  });
  return wrappedModel;
};

class PostHogAnthropic extends AnthropicOriginal {
  constructor(config) {
    const {
      posthog,
      ...anthropicConfig
    } = config;
    super(anthropicConfig);
    this.phClient = posthog;
    this.messages = new WrappedMessages(this, this.phClient);
  }
}
class WrappedMessages extends AnthropicOriginal.Messages {
  constructor(parentClient, phClient) {
    super(parentClient);
    this.phClient = phClient;
    this.baseURL = parentClient.baseURL;
  }
  create(body, options) {
    const {
      providerParams: anthropicParams,
      posthogParams
    } = extractPosthogParams(body);
    const startTime = Date.now();
    const parentPromise = super.create(anthropicParams, options);
    if (anthropicParams.stream) {
      return parentPromise.then(value => {
        let accumulatedContent = '';
        const contentBlocks = [];
        const toolsInProgress = new Map();
        let currentTextBlock = null;
        const usage = {
          inputTokens: 0,
          outputTokens: 0,
          cacheCreationInputTokens: 0,
          cacheReadInputTokens: 0
        };
        if ('tee' in value) {
          const [stream1, stream2] = value.tee();
          (async () => {
            try {
              for await (const chunk of stream1) {
                // Handle content block start events
                if (chunk.type === 'content_block_start') {
                  if (chunk.content_block?.type === 'text') {
                    currentTextBlock = {
                      type: 'text',
                      text: ''
                    };
                    contentBlocks.push(currentTextBlock);
                  } else if (chunk.content_block?.type === 'tool_use') {
                    const toolBlock = {
                      type: 'function',
                      id: chunk.content_block.id,
                      function: {
                        name: chunk.content_block.name,
                        arguments: {}
                      }
                    };
                    contentBlocks.push(toolBlock);
                    toolsInProgress.set(chunk.content_block.id, {
                      block: toolBlock,
                      inputString: ''
                    });
                    currentTextBlock = null;
                  }
                }
                // Handle text delta events
                if ('delta' in chunk) {
                  if ('text' in chunk.delta) {
                    const delta = chunk.delta.text;
                    accumulatedContent += delta;
                    if (currentTextBlock) {
                      currentTextBlock.text += delta;
                    }
                  }
                }
                // Handle tool input delta events
                if (chunk.type === 'content_block_delta' && chunk.delta?.type === 'input_json_delta') {
                  const block = chunk.index !== undefined ? contentBlocks[chunk.index] : undefined;
                  const toolId = block?.type === 'function' ? block.id : undefined;
                  if (toolId && toolsInProgress.has(toolId)) {
                    const tool = toolsInProgress.get(toolId);
                    if (tool) {
                      tool.inputString += chunk.delta.partial_json || '';
                    }
                  }
                }
                // Handle content block stop events
                if (chunk.type === 'content_block_stop') {
                  currentTextBlock = null;
                  // Parse accumulated tool input
                  if (chunk.index !== undefined) {
                    const block = contentBlocks[chunk.index];
                    if (block?.type === 'function' && block.id && toolsInProgress.has(block.id)) {
                      const tool = toolsInProgress.get(block.id);
                      if (tool) {
                        try {
                          block.function.arguments = JSON.parse(tool.inputString);
                        } catch (e) {
                          // Keep empty object if parsing fails
                          console.error('Error parsing tool input:', e);
                        }
                      }
                      toolsInProgress.delete(block.id);
                    }
                  }
                }
                if (chunk.type == 'message_start') {
                  usage.inputTokens = chunk.message.usage.input_tokens ?? 0;
                  usage.cacheCreationInputTokens = chunk.message.usage.cache_creation_input_tokens ?? 0;
                  usage.cacheReadInputTokens = chunk.message.usage.cache_read_input_tokens ?? 0;
                }
                if ('usage' in chunk) {
                  usage.outputTokens = chunk.usage.output_tokens ?? 0;
                }
              }
              const latency = (Date.now() - startTime) / 1000;
              const availableTools = extractAvailableToolCalls('anthropic', anthropicParams);
              // Format output to match non-streaming version
              const formattedOutput = contentBlocks.length > 0 ? [{
                role: 'assistant',
                content: contentBlocks
              }] : [{
                role: 'assistant',
                content: [{
                  type: 'text',
                  text: accumulatedContent
                }]
              }];
              await sendEventToPosthog({
                client: this.phClient,
                ...posthogParams,
                model: anthropicParams.model,
                provider: 'anthropic',
                input: sanitizeAnthropic(mergeSystemPrompt(anthropicParams, 'anthropic')),
                output: formattedOutput,
                latency,
                baseURL: this.baseURL,
                params: body,
                httpStatus: 200,
                usage,
                tools: availableTools
              });
            } catch (error) {
              // error handling
              await sendEventToPosthog({
                client: this.phClient,
                ...posthogParams,
                model: anthropicParams.model,
                provider: 'anthropic',
                input: sanitizeAnthropic(mergeSystemPrompt(anthropicParams)),
                output: [],
                latency: 0,
                baseURL: this.baseURL,
                params: body,
                httpStatus: error?.status ? error.status : 500,
                usage: {
                  inputTokens: 0,
                  outputTokens: 0
                },
                isError: true,
                error: JSON.stringify(error)
              });
            }
          })();
          // Return the other stream to the user
          return stream2;
        }
        return value;
      });
    } else {
      const wrappedPromise = parentPromise.then(async result => {
        if ('content' in result) {
          const latency = (Date.now() - startTime) / 1000;
          const availableTools = extractAvailableToolCalls('anthropic', anthropicParams);
          await sendEventToPosthog({
            client: this.phClient,
            ...posthogParams,
            model: anthropicParams.model,
            provider: 'anthropic',
            input: sanitizeAnthropic(mergeSystemPrompt(anthropicParams)),
            output: formatResponseAnthropic(result),
            latency,
            baseURL: this.baseURL,
            params: body,
            httpStatus: 200,
            usage: {
              inputTokens: result.usage.input_tokens ?? 0,
              outputTokens: result.usage.output_tokens ?? 0,
              cacheCreationInputTokens: result.usage.cache_creation_input_tokens ?? 0,
              cacheReadInputTokens: result.usage.cache_read_input_tokens ?? 0
            },
            tools: availableTools
          });
        }
        return result;
      }, async error => {
        await sendEventToPosthog({
          client: this.phClient,
          ...posthogParams,
          model: anthropicParams.model,
          provider: 'anthropic',
          input: sanitizeAnthropic(mergeSystemPrompt(anthropicParams)),
          output: [],
          latency: 0,
          baseURL: this.baseURL,
          params: body,
          httpStatus: error?.status ? error.status : 500,
          usage: {
            inputTokens: 0,
            outputTokens: 0
          },
          isError: true,
          error: JSON.stringify(error)
        });
        throw error;
      });
      return wrappedPromise;
    }
  }
}

class PostHogGoogleGenAI {
  constructor(config) {
    const {
      posthog,
      ...geminiConfig
    } = config;
    this.phClient = posthog;
    this.client = new GoogleGenAI(geminiConfig);
    this.models = new WrappedModels(this.client, this.phClient);
  }
}
class WrappedModels {
  constructor(client, phClient) {
    this.client = client;
    this.phClient = phClient;
  }
  async generateContent(params) {
    const {
      providerParams: geminiParams,
      posthogParams
    } = extractPosthogParams(params);
    const startTime = Date.now();
    try {
      const response = await this.client.models.generateContent(geminiParams);
      const latency = (Date.now() - startTime) / 1000;
      const availableTools = extractAvailableToolCalls('gemini', geminiParams);
      const metadata = response.usageMetadata;
      await sendEventToPosthog({
        client: this.phClient,
        ...posthogParams,
        model: geminiParams.model,
        provider: 'gemini',
        input: this.formatInputForPostHog(geminiParams.contents),
        output: formatResponseGemini(response),
        latency,
        baseURL: 'https://generativelanguage.googleapis.com',
        params: params,
        httpStatus: 200,
        usage: {
          inputTokens: metadata?.promptTokenCount ?? 0,
          outputTokens: metadata?.candidatesTokenCount ?? 0,
          reasoningTokens: metadata?.thoughtsTokenCount ?? 0,
          cacheReadInputTokens: metadata?.cachedContentTokenCount ?? 0
        },
        tools: availableTools
      });
      return response;
    } catch (error) {
      const latency = (Date.now() - startTime) / 1000;
      await sendEventToPosthog({
        client: this.phClient,
        ...posthogParams,
        model: geminiParams.model,
        provider: 'gemini',
        input: this.formatInputForPostHog(geminiParams.contents),
        output: [],
        latency,
        baseURL: 'https://generativelanguage.googleapis.com',
        params: params,
        httpStatus: error?.status ?? 500,
        usage: {
          inputTokens: 0,
          outputTokens: 0
        },
        isError: true,
        error: JSON.stringify(error)
      });
      throw error;
    }
  }
  async *generateContentStream(params) {
    const {
      providerParams: geminiParams,
      posthogParams
    } = extractPosthogParams(params);
    const startTime = Date.now();
    const accumulatedContent = [];
    let usage = {
      inputTokens: 0,
      outputTokens: 0
    };
    try {
      const stream = await this.client.models.generateContentStream(geminiParams);
      for await (const chunk of stream) {
        // Handle text content
        if (chunk.text) {
          // Find if we already have a text item to append to
          let lastTextItem;
          for (let i = accumulatedContent.length - 1; i >= 0; i--) {
            if (accumulatedContent[i].type === 'text') {
              lastTextItem = accumulatedContent[i];
              break;
            }
          }
          if (lastTextItem && lastTextItem.type === 'text') {
            lastTextItem.text += chunk.text;
          } else {
            accumulatedContent.push({
              type: 'text',
              text: chunk.text
            });
          }
        }
        // Handle function calls from candidates
        if (chunk.candidates && Array.isArray(chunk.candidates)) {
          for (const candidate of chunk.candidates) {
            if (candidate.content && candidate.content.parts) {
              for (const part of candidate.content.parts) {
                // Type-safe check for functionCall
                if ('functionCall' in part) {
                  const funcCall = part.functionCall;
                  if (funcCall?.name) {
                    accumulatedContent.push({
                      type: 'function',
                      function: {
                        name: funcCall.name,
                        arguments: funcCall.args || {}
                      }
                    });
                  }
                }
              }
            }
          }
        }
        // Update usage metadata - handle both old and new field names
        if (chunk.usageMetadata) {
          const metadata = chunk.usageMetadata;
          usage = {
            inputTokens: metadata.promptTokenCount ?? 0,
            outputTokens: metadata.candidatesTokenCount ?? 0,
            reasoningTokens: metadata.thoughtsTokenCount ?? 0,
            cacheReadInputTokens: metadata.cachedContentTokenCount ?? 0
          };
        }
        yield chunk;
      }
      const latency = (Date.now() - startTime) / 1000;
      const availableTools = extractAvailableToolCalls('gemini', geminiParams);
      // Format output similar to formatResponseGemini
      const output = accumulatedContent.length > 0 ? [{
        role: 'assistant',
        content: accumulatedContent
      }] : [];
      await sendEventToPosthog({
        client: this.phClient,
        ...posthogParams,
        model: geminiParams.model,
        provider: 'gemini',
        input: this.formatInputForPostHog(geminiParams.contents),
        output,
        latency,
        baseURL: 'https://generativelanguage.googleapis.com',
        params: params,
        httpStatus: 200,
        usage,
        tools: availableTools
      });
    } catch (error) {
      const latency = (Date.now() - startTime) / 1000;
      await sendEventToPosthog({
        client: this.phClient,
        ...posthogParams,
        model: geminiParams.model,
        provider: 'gemini',
        input: this.formatInputForPostHog(geminiParams.contents),
        output: [],
        latency,
        baseURL: 'https://generativelanguage.googleapis.com',
        params: params,
        httpStatus: error?.status ?? 500,
        usage: {
          inputTokens: 0,
          outputTokens: 0
        },
        isError: true,
        error: JSON.stringify(error)
      });
      throw error;
    }
  }
  formatInput(contents) {
    if (typeof contents === 'string') {
      return [{
        role: 'user',
        content: contents
      }];
    }
    if (Array.isArray(contents)) {
      return contents.map(item => {
        if (typeof item === 'string') {
          return {
            role: 'user',
            content: item
          };
        }
        if (item && typeof item === 'object') {
          const obj = item;
          if ('text' in obj && obj.text) {
            return {
              role: obj.role || 'user',
              content: obj.text
            };
          }
          if ('content' in obj && obj.content) {
            return {
              role: obj.role || 'user',
              content: obj.content
            };
          }
          if ('parts' in obj && Array.isArray(obj.parts)) {
            return {
              role: obj.role || 'user',
              content: obj.parts.map(part => {
                if (part && typeof part === 'object' && 'text' in part) {
                  return part.text;
                }
                return part;
              })
            };
          }
        }
        return {
          role: 'user',
          content: String(item)
        };
      });
    }
    if (contents && typeof contents === 'object') {
      const obj = contents;
      if ('text' in obj && obj.text) {
        return [{
          role: 'user',
          content: obj.text
        }];
      }
      if ('content' in obj && obj.content) {
        return [{
          role: 'user',
          content: obj.content
        }];
      }
    }
    return [{
      role: 'user',
      content: String(contents)
    }];
  }
  formatInputForPostHog(contents) {
    const sanitized = sanitizeGemini(contents);
    return this.formatInput(sanitized);
  }
}

function getDefaultExportFromCjs (x) {
	return x && x.__esModule && Object.prototype.hasOwnProperty.call(x, 'default') ? x['default'] : x;
}

var decamelize;
var hasRequiredDecamelize;

function requireDecamelize () {
	if (hasRequiredDecamelize) return decamelize;
	hasRequiredDecamelize = 1;
	decamelize = function (str, sep) {
		if (typeof str !== 'string') {
			throw new TypeError('Expected a string');
		}

		sep = typeof sep === 'undefined' ? '_' : sep;

		return str
			.replace(/([a-z\d])([A-Z])/g, '$1' + sep + '$2')
			.replace(/([A-Z]+)([A-Z][a-z\d]+)/g, '$1' + sep + '$2')
			.toLowerCase();
	};
	return decamelize;
}

var decamelizeExports = requireDecamelize();
var snakeCase = /*@__PURE__*/getDefaultExportFromCjs(decamelizeExports);

var camelcase = {exports: {}};

var hasRequiredCamelcase;

function requireCamelcase () {
	if (hasRequiredCamelcase) return camelcase.exports;
	hasRequiredCamelcase = 1;

	const UPPERCASE = /[\p{Lu}]/u;
	const LOWERCASE = /[\p{Ll}]/u;
	const LEADING_CAPITAL = /^[\p{Lu}](?![\p{Lu}])/gu;
	const IDENTIFIER = /([\p{Alpha}\p{N}_]|$)/u;
	const SEPARATORS = /[_.\- ]+/;

	const LEADING_SEPARATORS = new RegExp('^' + SEPARATORS.source);
	const SEPARATORS_AND_IDENTIFIER = new RegExp(SEPARATORS.source + IDENTIFIER.source, 'gu');
	const NUMBERS_AND_IDENTIFIER = new RegExp('\\d+' + IDENTIFIER.source, 'gu');

	const preserveCamelCase = (string, toLowerCase, toUpperCase) => {
		let isLastCharLower = false;
		let isLastCharUpper = false;
		let isLastLastCharUpper = false;

		for (let i = 0; i < string.length; i++) {
			const character = string[i];

			if (isLastCharLower && UPPERCASE.test(character)) {
				string = string.slice(0, i) + '-' + string.slice(i);
				isLastCharLower = false;
				isLastLastCharUpper = isLastCharUpper;
				isLastCharUpper = true;
				i++;
			} else if (isLastCharUpper && isLastLastCharUpper && LOWERCASE.test(character)) {
				string = string.slice(0, i - 1) + '-' + string.slice(i - 1);
				isLastLastCharUpper = isLastCharUpper;
				isLastCharUpper = false;
				isLastCharLower = true;
			} else {
				isLastCharLower = toLowerCase(character) === character && toUpperCase(character) !== character;
				isLastLastCharUpper = isLastCharUpper;
				isLastCharUpper = toUpperCase(character) === character && toLowerCase(character) !== character;
			}
		}

		return string;
	};

	const preserveConsecutiveUppercase = (input, toLowerCase) => {
		LEADING_CAPITAL.lastIndex = 0;

		return input.replace(LEADING_CAPITAL, m1 => toLowerCase(m1));
	};

	const postProcess = (input, toUpperCase) => {
		SEPARATORS_AND_IDENTIFIER.lastIndex = 0;
		NUMBERS_AND_IDENTIFIER.lastIndex = 0;

		return input.replace(SEPARATORS_AND_IDENTIFIER, (_, identifier) => toUpperCase(identifier))
			.replace(NUMBERS_AND_IDENTIFIER, m => toUpperCase(m));
	};

	const camelCase = (input, options) => {
		if (!(typeof input === 'string' || Array.isArray(input))) {
			throw new TypeError('Expected the input to be `string | string[]`');
		}

		options = {
			pascalCase: false,
			preserveConsecutiveUppercase: false,
			...options
		};

		if (Array.isArray(input)) {
			input = input.map(x => x.trim())
				.filter(x => x.length)
				.join('-');
		} else {
			input = input.trim();
		}

		if (input.length === 0) {
			return '';
		}

		const toLowerCase = options.locale === false ?
			string => string.toLowerCase() :
			string => string.toLocaleLowerCase(options.locale);
		const toUpperCase = options.locale === false ?
			string => string.toUpperCase() :
			string => string.toLocaleUpperCase(options.locale);

		if (input.length === 1) {
			return options.pascalCase ? toUpperCase(input) : toLowerCase(input);
		}

		const hasUpperCase = input !== toLowerCase(input);

		if (hasUpperCase) {
			input = preserveCamelCase(input, toLowerCase, toUpperCase);
		}

		input = input.replace(LEADING_SEPARATORS, '');

		if (options.preserveConsecutiveUppercase) {
			input = preserveConsecutiveUppercase(input, toLowerCase);
		} else {
			input = toLowerCase(input);
		}

		if (options.pascalCase) {
			input = toUpperCase(input.charAt(0)) + input.slice(1);
		}

		return postProcess(input, toUpperCase);
	};

	camelcase.exports = camelCase;
	// TODO: Remove this for the next major release
	camelcase.exports.default = camelCase;
	return camelcase.exports;
}

requireCamelcase();

function keyToJson(key, map) {
    return map?.[key] || snakeCase(key);
}
function mapKeys(fields, mapper, map) {
    const mapped = {};
    for (const key in fields) {
        if (Object.hasOwn(fields, key)) {
            mapped[mapper(key, map)] = fields[key];
        }
    }
    return mapped;
}

function shallowCopy(obj) {
    return Array.isArray(obj) ? [...obj] : { ...obj };
}
function replaceSecrets(root, secretsMap) {
    const result = shallowCopy(root);
    for (const [path, secretId] of Object.entries(secretsMap)) {
        const [last, ...partsReverse] = path.split(".").reverse();
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        let current = result;
        for (const part of partsReverse.reverse()) {
            if (current[part] === undefined) {
                break;
            }
            current[part] = shallowCopy(current[part]);
            current = current[part];
        }
        if (current[last] !== undefined) {
            current[last] = {
                lc: 1,
                type: "secret",
                id: [secretId],
            };
        }
    }
    return result;
}
/**
 * Get a unique name for the module, rather than parent class implementations.
 * Should not be subclassed, subclass lc_name above instead.
 */
function get_lc_unique_name(
// eslint-disable-next-line @typescript-eslint/no-use-before-define
serializableClass) {
    // "super" here would refer to the parent class of Serializable,
    // when we want the parent class of the module actually calling this method.
    const parentClass = Object.getPrototypeOf(serializableClass);
    const lcNameIsSubclassed = typeof serializableClass.lc_name === "function" &&
        (typeof parentClass.lc_name !== "function" ||
            serializableClass.lc_name() !== parentClass.lc_name());
    if (lcNameIsSubclassed) {
        return serializableClass.lc_name();
    }
    else {
        return serializableClass.name;
    }
}
class Serializable {
    /**
     * The name of the serializable. Override to provide an alias or
     * to preserve the serialized module name in minified environments.
     *
     * Implemented as a static method to support loading logic.
     */
    static lc_name() {
        return this.name;
    }
    /**
     * The final serialized identifier for the module.
     */
    get lc_id() {
        return [
            ...this.lc_namespace,
            get_lc_unique_name(this.constructor),
        ];
    }
    /**
     * A map of secrets, which will be omitted from serialization.
     * Keys are paths to the secret in constructor args, e.g. "foo.bar.baz".
     * Values are the secret ids, which will be used when deserializing.
     */
    get lc_secrets() {
        return undefined;
    }
    /**
     * A map of additional attributes to merge with constructor args.
     * Keys are the attribute names, e.g. "foo".
     * Values are the attribute values, which will be serialized.
     * These attributes need to be accepted by the constructor as arguments.
     */
    get lc_attributes() {
        return undefined;
    }
    /**
     * A map of aliases for constructor args.
     * Keys are the attribute names, e.g. "foo".
     * Values are the alias that will replace the key in serialization.
     * This is used to eg. make argument names match Python.
     */
    get lc_aliases() {
        return undefined;
    }
    /**
     * A manual list of keys that should be serialized.
     * If not overridden, all fields passed into the constructor will be serialized.
     */
    get lc_serializable_keys() {
        return undefined;
    }
    constructor(kwargs, ..._args) {
        Object.defineProperty(this, "lc_serializable", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "lc_kwargs", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        if (this.lc_serializable_keys !== undefined) {
            this.lc_kwargs = Object.fromEntries(Object.entries(kwargs || {}).filter(([key]) => this.lc_serializable_keys?.includes(key)));
        }
        else {
            this.lc_kwargs = kwargs ?? {};
        }
    }
    toJSON() {
        if (!this.lc_serializable) {
            return this.toJSONNotImplemented();
        }
        if (
        // eslint-disable-next-line no-instanceof/no-instanceof
        this.lc_kwargs instanceof Serializable ||
            typeof this.lc_kwargs !== "object" ||
            Array.isArray(this.lc_kwargs)) {
            // We do not support serialization of classes with arg not a POJO
            // I'm aware the check above isn't as strict as it could be
            return this.toJSONNotImplemented();
        }
        const aliases = {};
        const secrets = {};
        const kwargs = Object.keys(this.lc_kwargs).reduce((acc, key) => {
            acc[key] = key in this ? this[key] : this.lc_kwargs[key];
            return acc;
        }, {});
        // get secrets, attributes and aliases from all superclasses
        for (
        // eslint-disable-next-line @typescript-eslint/no-this-alias
        let current = Object.getPrototypeOf(this); current; current = Object.getPrototypeOf(current)) {
            Object.assign(aliases, Reflect.get(current, "lc_aliases", this));
            Object.assign(secrets, Reflect.get(current, "lc_secrets", this));
            Object.assign(kwargs, Reflect.get(current, "lc_attributes", this));
        }
        // include all secrets used, even if not in kwargs,
        // will be replaced with sentinel value in replaceSecrets
        Object.keys(secrets).forEach((keyPath) => {
            // eslint-disable-next-line @typescript-eslint/no-this-alias, @typescript-eslint/no-explicit-any
            let read = this;
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            let write = kwargs;
            const [last, ...partsReverse] = keyPath.split(".").reverse();
            for (const key of partsReverse.reverse()) {
                if (!(key in read) || read[key] === undefined)
                    return;
                if (!(key in write) || write[key] === undefined) {
                    if (typeof read[key] === "object" && read[key] != null) {
                        write[key] = {};
                    }
                    else if (Array.isArray(read[key])) {
                        write[key] = [];
                    }
                }
                read = read[key];
                write = write[key];
            }
            if (last in read && read[last] !== undefined) {
                write[last] = write[last] || read[last];
            }
        });
        return {
            lc: 1,
            type: "constructor",
            id: this.lc_id,
            kwargs: mapKeys(Object.keys(secrets).length ? replaceSecrets(kwargs, secrets) : kwargs, keyToJson, aliases),
        };
    }
    toJSONNotImplemented() {
        return {
            lc: 1,
            type: "not_implemented",
            id: this.lc_id,
        };
    }
}

// Supabase Edge Function provides a `Deno` global object
// without `version` property
const isDeno = () => typeof Deno !== "undefined";
function getEnvironmentVariable(name) {
    // Certain Deno setups will throw an error if you try to access environment variables
    // https://github.com/langchain-ai/langchainjs/issues/1412
    try {
        if (typeof process !== "undefined") {
            // eslint-disable-next-line no-process-env
            return process.env?.[name];
        }
        else if (isDeno()) {
            return Deno?.env.get(name);
        }
        else {
            return undefined;
        }
    }
    catch (e) {
        return undefined;
    }
}

/**
 * Abstract class that provides a set of optional methods that can be
 * overridden in derived classes to handle various events during the
 * execution of a LangChain application.
 */
class BaseCallbackHandlerMethodsClass {
}
/**
 * Abstract base class for creating callback handlers in the LangChain
 * framework. It provides a set of optional methods that can be overridden
 * in derived classes to handle various events during the execution of a
 * LangChain application.
 */
class BaseCallbackHandler extends BaseCallbackHandlerMethodsClass {
    get lc_namespace() {
        return ["langchain_core", "callbacks", this.name];
    }
    get lc_secrets() {
        return undefined;
    }
    get lc_attributes() {
        return undefined;
    }
    get lc_aliases() {
        return undefined;
    }
    get lc_serializable_keys() {
        return undefined;
    }
    /**
     * The name of the serializable. Override to provide an alias or
     * to preserve the serialized module name in minified environments.
     *
     * Implemented as a static method to support loading logic.
     */
    static lc_name() {
        return this.name;
    }
    /**
     * The final serialized identifier for the module.
     */
    get lc_id() {
        return [
            ...this.lc_namespace,
            get_lc_unique_name(this.constructor),
        ];
    }
    constructor(input) {
        super();
        Object.defineProperty(this, "lc_serializable", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "lc_kwargs", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "ignoreLLM", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "ignoreChain", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "ignoreAgent", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "ignoreRetriever", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "ignoreCustomEvent", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "raiseError", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "awaitHandlers", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: getEnvironmentVariable("LANGCHAIN_CALLBACKS_BACKGROUND") === "false"
        });
        this.lc_kwargs = input || {};
        if (input) {
            this.ignoreLLM = input.ignoreLLM ?? this.ignoreLLM;
            this.ignoreChain = input.ignoreChain ?? this.ignoreChain;
            this.ignoreAgent = input.ignoreAgent ?? this.ignoreAgent;
            this.ignoreRetriever = input.ignoreRetriever ?? this.ignoreRetriever;
            this.ignoreCustomEvent =
                input.ignoreCustomEvent ?? this.ignoreCustomEvent;
            this.raiseError = input.raiseError ?? this.raiseError;
            this.awaitHandlers =
                this.raiseError || (input._awaitHandler ?? this.awaitHandlers);
        }
    }
    copy() {
        return new this.constructor(this);
    }
    toJSON() {
        return Serializable.prototype.toJSON.call(this);
    }
    toJSONNotImplemented() {
        return Serializable.prototype.toJSONNotImplemented.call(this);
    }
    static fromMethods(methods) {
        class Handler extends BaseCallbackHandler {
            constructor() {
                super();
                Object.defineProperty(this, "name", {
                    enumerable: true,
                    configurable: true,
                    writable: true,
                    value: uuid.v4()
                });
                Object.assign(this, methods);
            }
        }
        return new Handler();
    }
}

class LangChainCallbackHandler extends BaseCallbackHandler {
  constructor(options) {
    if (!options.client) {
      throw new Error('PostHog client is required');
    }
    super();
    this.name = 'PosthogCallbackHandler';
    this.runs = {};
    this.parentTree = {};
    this.client = options.client;
    this.distinctId = options.distinctId;
    this.traceId = options.traceId;
    this.properties = options.properties || {};
    this.privacyMode = options.privacyMode || false;
    this.groups = options.groups || {};
    this.debug = options.debug || false;
  }
  // ===== CALLBACK METHODS =====
  handleChainStart(chain, inputs, runId, parentRunId, tags, metadata, _runType, runName) {
    this._logDebugEvent('on_chain_start', runId, parentRunId, {
      inputs,
      tags
    });
    this._setParentOfRun(runId, parentRunId);
    this._setTraceOrSpanMetadata(chain, inputs, runId, parentRunId, metadata, tags, runName);
  }
  handleChainEnd(outputs, runId, parentRunId, tags, _kwargs) {
    this._logDebugEvent('on_chain_end', runId, parentRunId, {
      outputs,
      tags
    });
    this._popRunAndCaptureTraceOrSpan(runId, parentRunId, outputs);
  }
  handleChainError(error, runId, parentRunId, tags, _kwargs) {
    this._logDebugEvent('on_chain_error', runId, parentRunId, {
      error,
      tags
    });
    this._popRunAndCaptureTraceOrSpan(runId, parentRunId, error);
  }
  handleChatModelStart(serialized, messages, runId, parentRunId, extraParams, tags, metadata, runName) {
    this._logDebugEvent('on_chat_model_start', runId, parentRunId, {
      messages,
      tags
    });
    this._setParentOfRun(runId, parentRunId);
    // Flatten the two-dimensional messages and convert each message to a plain object
    const input = messages.flat().map(m => this._convertMessageToDict(m));
    this._setLLMMetadata(serialized, runId, input, metadata, extraParams, runName);
  }
  handleLLMStart(serialized, prompts, runId, parentRunId, extraParams, tags, metadata, runName) {
    this._logDebugEvent('on_llm_start', runId, parentRunId, {
      prompts,
      tags
    });
    this._setParentOfRun(runId, parentRunId);
    this._setLLMMetadata(serialized, runId, prompts, metadata, extraParams, runName);
  }
  handleLLMEnd(output, runId, parentRunId, tags, _extraParams) {
    this._logDebugEvent('on_llm_end', runId, parentRunId, {
      output,
      tags
    });
    this._popRunAndCaptureGeneration(runId, parentRunId, output);
  }
  handleLLMError(err, runId, parentRunId, tags, _extraParams) {
    this._logDebugEvent('on_llm_error', runId, parentRunId, {
      err,
      tags
    });
    this._popRunAndCaptureGeneration(runId, parentRunId, err);
  }
  handleToolStart(tool, input, runId, parentRunId, tags, metadata, runName) {
    this._logDebugEvent('on_tool_start', runId, parentRunId, {
      input,
      tags
    });
    this._setParentOfRun(runId, parentRunId);
    this._setTraceOrSpanMetadata(tool, input, runId, parentRunId, metadata, tags, runName);
  }
  handleToolEnd(output, runId, parentRunId, tags) {
    this._logDebugEvent('on_tool_end', runId, parentRunId, {
      output,
      tags
    });
    this._popRunAndCaptureTraceOrSpan(runId, parentRunId, output);
  }
  handleToolError(err, runId, parentRunId, tags) {
    this._logDebugEvent('on_tool_error', runId, parentRunId, {
      err,
      tags
    });
    this._popRunAndCaptureTraceOrSpan(runId, parentRunId, err);
  }
  handleRetrieverStart(retriever, query, runId, parentRunId, tags, metadata, name) {
    this._logDebugEvent('on_retriever_start', runId, parentRunId, {
      query,
      tags
    });
    this._setParentOfRun(runId, parentRunId);
    this._setTraceOrSpanMetadata(retriever, query, runId, parentRunId, metadata, tags, name);
  }
  handleRetrieverEnd(documents, runId, parentRunId, tags) {
    this._logDebugEvent('on_retriever_end', runId, parentRunId, {
      documents,
      tags
    });
    this._popRunAndCaptureTraceOrSpan(runId, parentRunId, documents);
  }
  handleRetrieverError(err, runId, parentRunId, tags) {
    this._logDebugEvent('on_retriever_error', runId, parentRunId, {
      err,
      tags
    });
    this._popRunAndCaptureTraceOrSpan(runId, parentRunId, err);
  }
  handleAgentAction(action, runId, parentRunId, tags) {
    this._logDebugEvent('on_agent_action', runId, parentRunId, {
      action,
      tags
    });
    this._setParentOfRun(runId, parentRunId);
    this._setTraceOrSpanMetadata(null, action, runId, parentRunId);
  }
  handleAgentEnd(action, runId, parentRunId, tags) {
    this._logDebugEvent('on_agent_finish', runId, parentRunId, {
      action,
      tags
    });
    this._popRunAndCaptureTraceOrSpan(runId, parentRunId, action);
  }
  // ===== PRIVATE HELPERS =====
  _setParentOfRun(runId, parentRunId) {
    if (parentRunId) {
      this.parentTree[runId] = parentRunId;
    }
  }
  _popParentOfRun(runId) {
    delete this.parentTree[runId];
  }
  _findRootRun(runId) {
    let id = runId;
    while (this.parentTree[id]) {
      id = this.parentTree[id];
    }
    return id;
  }
  _setTraceOrSpanMetadata(serialized, input, runId, parentRunId, ...args) {
    // Use default names if not provided: if this is a top-level run, we mark it as a trace, otherwise as a span.
    const defaultName = parentRunId ? 'span' : 'trace';
    const runName = this._getLangchainRunName(serialized, ...args) || defaultName;
    this.runs[runId] = {
      name: runName,
      input,
      startTime: Date.now()
    };
  }
  _setLLMMetadata(serialized, runId, messages, metadata, extraParams, runName) {
    const runNameFound = this._getLangchainRunName(serialized, {
      extraParams,
      runName
    }) || 'generation';
    const generation = {
      name: runNameFound,
      input: sanitizeLangChain(messages),
      startTime: Date.now()
    };
    if (extraParams) {
      generation.modelParams = getModelParams(extraParams.invocation_params);
      if (extraParams.invocation_params && extraParams.invocation_params.tools) {
        generation.tools = extraParams.invocation_params.tools;
      }
    }
    if (metadata) {
      if (metadata.ls_model_name) {
        generation.model = metadata.ls_model_name;
      }
      if (metadata.ls_provider) {
        generation.provider = metadata.ls_provider;
      }
    }
    if (serialized && 'kwargs' in serialized && serialized.kwargs.openai_api_base) {
      generation.baseUrl = serialized.kwargs.openai_api_base;
    }
    this.runs[runId] = generation;
  }
  _popRunMetadata(runId) {
    const endTime = Date.now();
    const run = this.runs[runId];
    if (!run) {
      console.warn(`No run metadata found for run ${runId}`);
      return undefined;
    }
    run.endTime = endTime;
    delete this.runs[runId];
    return run;
  }
  _getTraceId(runId) {
    return this.traceId ? String(this.traceId) : this._findRootRun(runId);
  }
  _getParentRunId(traceId, _runId, parentRunId) {
    // Replace the parent-run if not found in our stored parent tree.
    if (parentRunId && !this.parentTree[parentRunId]) {
      return traceId;
    }
    return parentRunId;
  }
  _popRunAndCaptureTraceOrSpan(runId, parentRunId, outputs) {
    const traceId = this._getTraceId(runId);
    this._popParentOfRun(runId);
    const run = this._popRunMetadata(runId);
    if (!run) {
      return;
    }
    if ('modelParams' in run) {
      console.warn(`Run ${runId} is a generation, but attempted to be captured as a trace/span.`);
      return;
    }
    const actualParentRunId = this._getParentRunId(traceId, runId, parentRunId);
    this._captureTraceOrSpan(traceId, runId, run, outputs, actualParentRunId);
  }
  _captureTraceOrSpan(traceId, runId, run, outputs, parentRunId) {
    const eventName = parentRunId ? '$ai_span' : '$ai_trace';
    const latency = run.endTime ? (run.endTime - run.startTime) / 1000 : 0;
    const eventProperties = {
      $ai_lib: 'posthog-ai',
      $ai_lib_version: version,
      $ai_trace_id: traceId,
      $ai_input_state: withPrivacyMode(this.client, this.privacyMode, run.input),
      $ai_latency: latency,
      $ai_span_name: run.name,
      $ai_span_id: runId
    };
    if (parentRunId) {
      eventProperties['$ai_parent_id'] = parentRunId;
    }
    Object.assign(eventProperties, this.properties);
    if (!this.distinctId) {
      eventProperties['$process_person_profile'] = false;
    }
    if (outputs instanceof Error) {
      eventProperties['$ai_error'] = outputs.toString();
      eventProperties['$ai_is_error'] = true;
    } else if (outputs !== undefined) {
      eventProperties['$ai_output_state'] = withPrivacyMode(this.client, this.privacyMode, outputs);
    }
    this.client.capture({
      distinctId: this.distinctId ? this.distinctId.toString() : runId,
      event: eventName,
      properties: eventProperties,
      groups: this.groups
    });
  }
  _popRunAndCaptureGeneration(runId, parentRunId, response) {
    const traceId = this._getTraceId(runId);
    this._popParentOfRun(runId);
    const run = this._popRunMetadata(runId);
    if (!run || typeof run !== 'object' || !('modelParams' in run)) {
      console.warn(`Run ${runId} is not a generation, but attempted to be captured as such.`);
      return;
    }
    const actualParentRunId = this._getParentRunId(traceId, runId, parentRunId);
    this._captureGeneration(traceId, runId, run, response, actualParentRunId);
  }
  _captureGeneration(traceId, runId, run, output, parentRunId) {
    const latency = run.endTime ? (run.endTime - run.startTime) / 1000 : 0;
    const eventProperties = {
      $ai_lib: 'posthog-ai',
      $ai_lib_version: version,
      $ai_trace_id: traceId,
      $ai_span_id: runId,
      $ai_span_name: run.name,
      $ai_parent_id: parentRunId,
      $ai_provider: run.provider,
      $ai_model: run.model,
      $ai_model_parameters: run.modelParams,
      $ai_input: withPrivacyMode(this.client, this.privacyMode, run.input),
      $ai_http_status: 200,
      $ai_latency: latency,
      $ai_base_url: run.baseUrl
    };
    if (run.tools) {
      eventProperties['$ai_tools'] = run.tools;
    }
    if (output instanceof Error) {
      eventProperties['$ai_http_status'] = output.status || 500;
      eventProperties['$ai_error'] = output.toString();
      eventProperties['$ai_is_error'] = true;
    } else {
      // Handle token usage
      const [inputTokens, outputTokens, additionalTokenData] = this.parseUsage(output);
      eventProperties['$ai_input_tokens'] = inputTokens;
      eventProperties['$ai_output_tokens'] = outputTokens;
      // Add additional token data to properties
      if (additionalTokenData.cacheReadInputTokens) {
        eventProperties['$ai_cache_read_tokens'] = additionalTokenData.cacheReadInputTokens;
      }
      if (additionalTokenData.reasoningTokens) {
        eventProperties['$ai_reasoning_tokens'] = additionalTokenData.reasoningTokens;
      }
      // Handle generations/completions
      let completions;
      if (output.generations && Array.isArray(output.generations)) {
        const lastGeneration = output.generations[output.generations.length - 1];
        if (Array.isArray(lastGeneration) && lastGeneration.length > 0) {
          // Check if this is a ChatGeneration by looking at the first item
          const isChatGeneration = 'message' in lastGeneration[0] && lastGeneration[0].message;
          if (isChatGeneration) {
            // For ChatGeneration, convert messages to dict format
            completions = lastGeneration.map(gen => {
              return this._convertMessageToDict(gen.message);
            });
          } else {
            // For non-ChatGeneration, extract raw response
            completions = lastGeneration.map(gen => {
              return this._extractRawResponse(gen);
            });
          }
        }
      }
      if (completions) {
        eventProperties['$ai_output_choices'] = withPrivacyMode(this.client, this.privacyMode, completions);
      }
    }
    Object.assign(eventProperties, this.properties);
    if (!this.distinctId) {
      eventProperties['$process_person_profile'] = false;
    }
    this.client.capture({
      distinctId: this.distinctId ? this.distinctId.toString() : traceId,
      event: '$ai_generation',
      properties: eventProperties,
      groups: this.groups
    });
  }
  _logDebugEvent(eventName, runId, parentRunId, extra) {
    if (this.debug) {
      console.log(`Event: ${eventName}, runId: ${runId}, parentRunId: ${parentRunId}, extra:`, extra);
    }
  }
  _getLangchainRunName(serialized, ...args) {
    if (args && args.length > 0) {
      for (const arg of args) {
        if (arg && typeof arg === 'object' && 'name' in arg) {
          return arg.name;
        } else if (arg && typeof arg === 'object' && 'runName' in arg) {
          return arg.runName;
        }
      }
    }
    if (serialized && serialized.name) {
      return serialized.name;
    }
    if (serialized && serialized.id) {
      return Array.isArray(serialized.id) ? serialized.id[serialized.id.length - 1] : serialized.id;
    }
    return undefined;
  }
  _convertLcToolCallsToOai(toolCalls) {
    return toolCalls.map(toolCall => ({
      type: 'function',
      id: toolCall.id,
      function: {
        name: toolCall.name,
        arguments: JSON.stringify(toolCall.args)
      }
    }));
  }
  _extractRawResponse(generation) {
    // Extract the response from the last response of the LLM call
    // We return the text of the response if not empty
    if (generation.text != null && generation.text.trim() !== '') {
      return generation.text.trim();
    } else if (generation.message) {
      // Additional kwargs contains the response in case of tool usage
      return generation.message.additional_kwargs || generation.message.additionalKwargs || {};
    } else {
      // Not tool usage, some LLM responses can be simply empty
      return '';
    }
  }
  _convertMessageToDict(message) {
    let messageDict = {};
    const messageType = message.getType();
    switch (messageType) {
      case 'human':
        messageDict = {
          role: 'user',
          content: message.content
        };
        break;
      case 'ai':
        messageDict = {
          role: 'assistant',
          content: message.content
        };
        if (message.tool_calls) {
          messageDict.tool_calls = this._convertLcToolCallsToOai(message.tool_calls);
        }
        break;
      case 'system':
        messageDict = {
          role: 'system',
          content: message.content
        };
        break;
      case 'tool':
        messageDict = {
          role: 'tool',
          content: message.content
        };
        break;
      case 'function':
        messageDict = {
          role: 'function',
          content: message.content
        };
        break;
      default:
        messageDict = {
          role: messageType,
          content: String(message.content)
        };
        break;
    }
    if (message.additional_kwargs) {
      messageDict = {
        ...messageDict,
        ...message.additional_kwargs
      };
    }
    // Sanitize the message content to redact base64 images
    return sanitizeLangChain(messageDict);
  }
  _parseUsageModel(usage) {
    const conversionList = [['promptTokens', 'input'], ['completionTokens', 'output'], ['input_tokens', 'input'], ['output_tokens', 'output'], ['prompt_token_count', 'input'], ['candidates_token_count', 'output'], ['inputTokenCount', 'input'], ['outputTokenCount', 'output'], ['input_token_count', 'input'], ['generated_token_count', 'output']];
    const parsedUsage = conversionList.reduce((acc, [modelKey, typeKey]) => {
      const value = usage[modelKey];
      if (value != null) {
        const finalCount = Array.isArray(value) ? value.reduce((sum, tokenCount) => sum + tokenCount, 0) : value;
        acc[typeKey] = finalCount;
      }
      return acc;
    }, {
      input: 0,
      output: 0
    });
    // Extract additional token details like cached tokens and reasoning tokens
    const additionalTokenData = {};
    // Check for cached tokens in various formats
    if (usage.prompt_tokens_details?.cached_tokens != null) {
      additionalTokenData.cacheReadInputTokens = usage.prompt_tokens_details.cached_tokens;
    } else if (usage.input_token_details?.cache_read != null) {
      additionalTokenData.cacheReadInputTokens = usage.input_token_details.cache_read;
    } else if (usage.cachedPromptTokens != null) {
      additionalTokenData.cacheReadInputTokens = usage.cachedPromptTokens;
    }
    // Check for reasoning tokens in various formats
    if (usage.completion_tokens_details?.reasoning_tokens != null) {
      additionalTokenData.reasoningTokens = usage.completion_tokens_details.reasoning_tokens;
    } else if (usage.output_token_details?.reasoning != null) {
      additionalTokenData.reasoningTokens = usage.output_token_details.reasoning;
    } else if (usage.reasoningTokens != null) {
      additionalTokenData.reasoningTokens = usage.reasoningTokens;
    }
    return [parsedUsage.input, parsedUsage.output, additionalTokenData];
  }
  parseUsage(response) {
    let llmUsage = [0, 0, {}];
    const llmUsageKeys = ['token_usage', 'usage', 'tokenUsage'];
    if (response.llmOutput != null) {
      const key = llmUsageKeys.find(k => response.llmOutput?.[k] != null);
      if (key) {
        llmUsage = this._parseUsageModel(response.llmOutput[key]);
      }
    }
    // If top-level usage info was not found, try checking the generations.
    if (llmUsage[0] === 0 && llmUsage[1] === 0 && response.generations) {
      for (const generation of response.generations) {
        for (const genChunk of generation) {
          // Check other paths for usage information
          if (genChunk.generationInfo?.usage_metadata) {
            llmUsage = this._parseUsageModel(genChunk.generationInfo.usage_metadata);
            return llmUsage;
          }
          const messageChunk = genChunk.generationInfo ?? {};
          const responseMetadata = messageChunk.response_metadata ?? {};
          const chunkUsage = responseMetadata['usage'] ?? responseMetadata['amazon-bedrock-invocationMetrics'] ?? messageChunk.usage_metadata;
          if (chunkUsage) {
            llmUsage = this._parseUsageModel(chunkUsage);
            return llmUsage;
          }
        }
      }
    }
    return llmUsage;
  }
}

export { PostHogAnthropic as Anthropic, PostHogAzureOpenAI as AzureOpenAI, PostHogGoogleGenAI as GoogleGenAI, LangChainCallbackHandler, PostHogOpenAI as OpenAI, wrapVercelLanguageModel as withTracing };
//# sourceMappingURL=index.mjs.map
